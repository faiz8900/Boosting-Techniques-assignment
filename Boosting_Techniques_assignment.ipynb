{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Boosting Techniques**"
      ],
      "metadata": {
        "id": "ElFGVBEUnCbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n"
      ],
      "metadata": {
        "id": "28ohzBe_nFSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boosting is an iterative ensemble learning technique where multiple simple \"weak\" learners are combined sequentially to form a single, highly accurate \"strong\" learner. It improves weak learners by focusing each new model on the data points that previous models misclassified or struggled with, effectively assigning greater weight to these difficult samples. This process continues until the model reaches a desired accuracy, creating a model that can capture complex patterns and has superior predictive power."
      ],
      "metadata": {
        "id": "kGmXLwDgnKgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?"
      ],
      "metadata": {
        "id": "zEwLLqtJnQVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- AdaBoost improves weak learners by assigning higher weights to misclassified data points in subsequent training iterations, creating a sequence of weighted models that are combined through a weighted vote. In contrast, Gradient Boosting sequentially trains new models to predict the residuals (errors) of the previous models, using gradient descent to minimize the overall loss function. While both are sequential ensemble methods, AdaBoost focuses on re-weighting the data, whereas Gradient Boosting focuses on correcting errors by fitting models to the negative gradient of the loss function."
      ],
      "metadata": {
        "id": "HhnfVv2inUHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How does regularization help in XGBoost?\n"
      ],
      "metadata": {
        "id": "orWHSrLxnZ2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Here's how regularization helps in XGBoost:\n",
        "Preventing Overfitting:\n",
        "Regularization penalizes large leaf weights and a high number of leaves in the decision trees. This discourages the model from creating very deep or complex trees that might memorize the training data rather than learning generalizable patterns.\n",
        "Improving Generalization:\n",
        "By controlling complexity, regularization ensures that the model performs well on unseen data, not just the data it was trained on. This leads to more robust and reliable predictions.\n",
        "Controlling Model Complexity:\n",
        "XGBoost incorporates several regularization parameters:\n",
        "Gamma (γ): This parameter controls the minimum loss reduction required to make a further partition on a leaf node of the tree. A higher gamma value leads to more conservative pruning, resulting in simpler trees.\n",
        "Lambda (λ) / L2 Regularization: This adds a penalty proportional to the square of the leaf weights. It encourages smaller, more distributed leaf weights, reducing the impact of individual data points.\n",
        "Alpha (α) / L1 Regularization: This adds a penalty proportional to the absolute value of the leaf weights. It can lead to sparsity by driving some leaf weights to zero, effectively performing feature selection and simplifying the model.\n",
        "By carefully tuning these regularization parameters, users can find a balance between model complexity and predictive performance, leading to more effective and generalizable XGBoost models."
      ],
      "metadata": {
        "id": "TXsxGNBdnfuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why is CatBoost considered efficient for handling categorical data?"
      ],
      "metadata": {
        "id": "s2wjVOdUnqXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CatBoost is considered efficient for handling categorical data primarily due to its innovative and automatic internal mechanisms that address the challenges posed by such features.\n",
        "Ordered Target Encoding:\n",
        "CatBoost employs a technique called Ordered Target Encoding to transform categorical features into numerical representations. Unlike traditional methods like one-hot encoding or simple target encoding, CatBoost calculates target statistics (e.g., mean of the target variable for each category) in a specific, ordered manner to prevent target leakage and reduce overfitting. This sequential calculation ensures that future data points do not influence the encoding of current ones.\n",
        "Ordered Boosting:\n",
        "CatBoost introduces \"Ordered Boosting,\" a variation of gradient boosting where the calculation of residuals and the construction of new trees are done in a specific order. This ordering further mitigates the risk of target leakage, which can be particularly problematic when dealing with categorical features and can lead to overly optimistic performance estimates."
      ],
      "metadata": {
        "id": "1j5nU4zVnv_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n"
      ],
      "metadata": {
        "id": "hL36DSOhn6r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boosting techniques are preferred over bagging when the goal is to minimize bias and achieve a highly accurate model, especially for datasets with a high bias and low variance, such as in customer churn prediction, financial forecasting, and medical diagnosis where identifying subtle patterns is crucial. Boosting excels by sequentially training weak learners to focus on the mistakes of previous models, leading to superior predictive power in complex problems, though it requires careful tuning to avoid overfitting."
      ],
      "metadata": {
        "id": "4cVXmHNroAiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n"
      ],
      "metadata": {
        "id": "4uiHZ4U2oOuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an AdaBoost Classifier\n",
        "adaboost = AdaBoostClassifier(random_state=42)\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcDTLI6hoc97",
        "outputId": "b396ca7d-bd33-4f4e-ba81-73433f9706f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n"
      ],
      "metadata": {
        "id": "07qdnLECo5wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1CXn2gKo_q8",
        "outputId": "e5d1ae4b-c1f5-4160-c255-be87b35b690c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "XydrL-_iqKxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the parameter grid for learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Perform GridSearchCV to find the best learning_rate\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best accuracy found by GridSearchCV\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best accuracy (training): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5FWZ5myrGop",
        "outputId": "72669469-4199-4fd6-cf3f-7b2a66d4b185"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'learning_rate': 0.2}\n",
            "Best accuracy (training): 0.9670\n",
            "Test accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [17:57:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "vTTftlCgrWqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the CatBoost Classifier\n",
        "catboost = CatBoostClassifier(verbose=0, random_state=42) # Set verbose=0 to reduce output\n",
        "catboost.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = catboost.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix for CatBoost Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFoTUZ7vsI91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ldsY_HGssbUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model"
      ],
      "metadata": {
        "id": "ibZG2qimsb-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To predict loan defaults in an imbalanced, multi-feature dataset, your data science pipeline involves: (1) Preprocessing: Handle missing values using imputation, encode categorical features (e.g., one-hot encoding), and address imbalance with oversampling (e.g., SMOTE) or undersampling. (2) Model Choice: XGBoost or CatBoost are suitable; CatBoost excels with categorical features, while XGBoost offers speed and performance for large datasets. (3) Tuning: Use Grid Search or Random Search with cross-validation on metrics like AUC-ROC and F1-score to find optimal hyperparameters. (4) Evaluation: Utilize precision, recall, and F1-score to measure how well the model identifies defaults (true positives), especially due to class imbalance. (5) Business Benefit: The model helps Lendingkart reduce default risk, improve loan underwriting, and target high-risk customers with proactive strategies, leading to lower losses and more sustainable lending"
      ],
      "metadata": {
        "id": "C5L4U11Vshin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "Handle Missing Values:\n",
        "Implement imputation techniques for missing demographic or transaction data, such as mean/median imputation for numerical features or using a predictive model for imputation if missingness is systematic.\n",
        "Encode Categorical Features:\n",
        "Convert categorical data (e.g., income bracket, job title) into a numerical format suitable for boosting models. One-Hot Encoding or Target Encoding are effective choices."
      ],
      "metadata": {
        "id": "ObtftGWds-WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2. Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "XGBoost:\n",
        "An extremely popular and robust gradient boosting algorithm known for its efficiency, speed, and ability to handle large datasets effectively, often providing high accuracy.\n",
        "CatBoost:\n",
        "Particularly strong at handling categorical features directly, making it a great choice when you have many such features, and it often delivers high performance with less feature engineering compared to other boosters."
      ],
      "metadata": {
        "id": "HNMpoDCCtDB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3. Hyperparameter Tuning Strategy\n",
        "Cross-Validation:\n",
        "Split your preprocessed data into training and validation sets and use k-fold cross-validation to ensure the model's performance is robust and not overly dependent on a specific data split.\n",
        "Hyperparameter Optimization:\n",
        "Employ grid search or random search to systematically explore a range of hyperparameters (e.g., learning_rate, n_estimators, max_depth) to find the combination that yields the best performance on your validation set."
      ],
      "metadata": {
        "id": "dI8Y4vXztHDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4. Evaluation Metrics\n",
        "Precision and Recall:\n",
        "Crucial for imbalanced datasets.\n",
        "Precision: Measures the proportion of predicted defaults that were actually defaults. High precision minimizes the risk of falsely denying a loan to a solvent customer.\n",
        "Recall: Measures the proportion of actual defaults that were correctly identified. High recall is essential for accurately identifying high-risk borrowers and preventing losses."
      ],
      "metadata": {
        "id": "w2mT8syatLZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- F1-Score:\n",
        "The harmonic mean of precision and recall, providing a single metric that balances both. This is vital to get a balanced view of the model's performance when dealing with class imbalance."
      ],
      "metadata": {
        "id": "dtrcImQttPPi"
      }
    }
  ]
}